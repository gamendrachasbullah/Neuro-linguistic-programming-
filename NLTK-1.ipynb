{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['two', 'plus', 'two', 'is', 'four']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_string = \"Two plus two is four, minus one that's three — quick maths. Every day man's on the block. Smoke trees. See your girl in the park, that girl is an uckers. When the thing went quack quack quack, your men were ducking! Hold tight Asznee, my brother. He's got a pumpy. Hold tight my man, my guy. He's got a frisbee. I trap, trap, trap on the phone. Moving that cornflakes, rice crispies. Hold tight my girl Whitney.\"\n",
    "tokens = word_tokenize(my_string)\n",
    "tokens = [word.lower() for word in tokens]\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gc/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: 'U' mode is deprecated\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "f = open('walden.txt','rU')\n",
    "raw = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Two plus two is four, minus one that's three — quick maths.\", \"Every day man's on the block.\", 'Smoke trees.', 'See your girl in the park, that girl is an uckers.', 'When the thing went quack quack quack, your men were ducking!', 'Hold tight Asznee, my brother.', \"He's got a pumpy.\", 'Hold tight my man, my guy.', \"He's got a frisbee.\", 'I trap, trap, trap on the phone.', 'Moving that cornflakes, rice crispies.', 'Hold tight my girl Whitney.']\n"
     ]
    }
   ],
   "source": [
    "print(sent_tokenize(my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'so', 'happpy']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "Tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "tweet = \"@Kirana_Sutanto I am so happppppy\"\n",
    "print(Tokenizer.tokenize(tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Text(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      " girl in the park , that girl is an uckers . when the thing went quack quack q\n"
     ]
    }
   ],
   "source": [
    "t.concordance('uckers') # concordance() is a method of the Text class of NLTK. It finds words and displays a context window. Word matching is not case-sensitive.\n",
    "# concordance() is defined as follows: concordance(self, word, width=79, lines=25). Note default values for optional params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hold tight; quack quack\n"
     ]
    }
   ],
   "source": [
    "t.collocations() # def collocations(self, num=20, window_size=2). num is the max no. of collocations to print.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.count('quack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.index('two')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "guy\n"
     ]
    }
   ],
   "source": [
    "t.similar('brother')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'.': 11, ',': 8, \"'s\": 4, 'the': 4, 'my': 4, 'that': 3, 'girl': 3, 'quack': 3, 'hold': 3, 'tight': 3, ...})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Two', 'plus', 'two', 'is', 'four', ',', 'minus', 'one', 'that', \"'\", 's', 'three', '—', 'quick', 'maths', '.', 'Every', 'day', 'man', \"'\", 's', 'on', 'the', 'block', '.', 'Smoke', 'trees', '.', 'See', 'your', 'girl', 'in', 'the', 'park', ',', 'that', 'girl', 'is', 'an', 'uckers', '.', 'When', 'the', 'thing', 'went', 'quack', 'quack', 'quack', ',', 'your', 'men', 'were', 'ducking', '!', 'Hold', 'tight', 'Asznee', ',', 'my', 'brother', '.', 'He', \"'\", 's', 'got', 'a', 'pumpy', '.', 'Hold', 'tight', 'my', 'man', ',', 'my', 'guy', '.', 'He', \"'\", 's', 'got', 'a', 'frisbee', '.', 'I', 'trap', ',', 'trap', ',', 'trap', 'on', 'the', 'phone', '.', 'Moving', 'that', 'cornflakes', ',', 'rice', 'crispies', '.', 'Hold', 'tight', 'my', 'girl', 'Whitney', '.']\n"
     ]
    }
   ],
   "source": [
    "print (wordpunct_tokenize(my_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stopwords Corpus  This corpus contains lists of stop words for several languages.  These are high-frequency grammatical words which are usually ignored in text retrieval applications.  They were obtained from: http://anoncvs.postgresql.org/cvsweb.cgi/pgsql/src/backend/snowball/stopwords/  The stop words for the Romanian language were obtained from: http://arlc.ro/resources/  The English list has been augmented https://github.com/nltk/nltk_data/issues/22  The German list has been corrected https://github.com/nltk/nltk_data/pull/49  A Kazakh list has been added https://github.com/nltk/nltk_data/pull/52  A Nepali list has been added https://github.com/nltk/nltk_data/pull/83  An Azerbaijani list has been added https://github.com/nltk/nltk_data/pull/100  A Greek list has been added https://github.com/nltk/nltk_data/pull/103  An Indonesian list has been added https://github.com/nltk/nltk_data/pull/112 '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.readme().replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['arabic',\n",
       " 'azerbaijani',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'greek',\n",
       " 'hungarian',\n",
       " 'indonesian',\n",
       " 'italian',\n",
       " 'kazakh',\n",
       " 'nepali',\n",
       " 'norwegian',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'slovene',\n",
       " 'spanish',\n",
       " 'swedish',\n",
       " 'tajik',\n",
       " 'turkish']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopwords.words(['english']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'danish': 3,\n",
       " 'dutch': 3,\n",
       " 'english': 13,\n",
       " 'finnish': 2,\n",
       " 'french': 1,\n",
       " 'german': 3,\n",
       " 'hungarian': 1,\n",
       " 'italian': 3,\n",
       " 'norwegian': 3,\n",
       " 'portuguese': 1,\n",
       " 'russian': 0,\n",
       " 'spanish': 2,\n",
       " 'swedish': 3,\n",
       " 'turkish': 0}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_ratios = {}\n",
    "\n",
    "test_words = [word.lower() for word in tokens] # lowercase all tokens\n",
    "test_words_set = set(test_words)\n",
    "\n",
    "for language in stopwords.fileids():\n",
    "    stopwords_set = set(stopwords.words(language)) # For some languages eg. Russian, it would be a wise idea to tokenize the stop words by punctuation too.\n",
    "    common_elements = test_words_set.intersection(stopwords_set)\n",
    "    language_ratios[language] = len(common_elements) # language \"score\"\n",
    "    \n",
    "language_ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english\n"
     ]
    }
   ],
   "source": [
    "most_rated_language = max(language_ratios, key=language_ratios.get)\n",
    "print (most_rated_language)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'an',\n",
       " 'he',\n",
       " 'i',\n",
       " 'in',\n",
       " 'is',\n",
       " 'my',\n",
       " 'on',\n",
       " 'that',\n",
       " 'the',\n",
       " 'were',\n",
       " 'when',\n",
       " 'your'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words_set.intersection(set(stopwords.words(most_rated_language)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
